# -*- coding: utf-8 -*-
"""Untitled7.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PJtNIfxWXfEbP5hdbPVXKOMYw-4EBiP1
"""

import pandas as pd
import numpy as np
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from keras.models import Sequential
from keras.layers import Dense

#print(df['make'].value_counts())

# for col in x.columns:
#     print(col)


# Evaluate the model

data = pd.read_csv("/content/drive/MyDrive/AviationData.csv",encoding='cp1252')

df = data[['Weather_Condition','Aircraft_damage','Aircraft_Category','Make','distance']]
df.dropna(inplace= True, how='any')

# for val,ind in enumerate(df['Latitude']):import pandas as pd
import numpy as np
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from keras.models import Sequential
from keras.layers import Dense

#print(df['make'].value_counts())

# for col in x.columns:
#     print(col)


# Evaluate the model
#   if str(val).endswith('N'):
#     df[ind]['Latitude'] =int(val[0])*10+int(val[1])+(int(val[2])*10+int(val[3]))/60
#   if str(val).endswith('S'):
#     df[ind]['Latitude'] =int(val[0])*10+int(val[1])+(int(val[2])*10+int(val[3]))/60*-1
    
print(df)

# for col in x:
#   print(col,sum(col))

from google.colab import drive
drive.mount('/content/drive')

k = df.Make.value_counts().to_dict()
arr = []
for i in k:
  if(k[i] < 20):
      arr.append(i)
print(arr)
print(df)
for i in arr:
  df = df.drop(df[df['Make'] == i].index)
print(df)

df = pd.get_dummies(df, columns = ['Aircraft_damage','Weather_Condition','Aircraft_Category','Make'])

x = df.drop(['distance','Aircraft_damage_Unknown'], axis=1)
y = df['distance']
print(x,y)

x = x.values #returns a numpy array
min_max_scaler = preprocessing.MinMaxScaler()
x_scaled = min_max_scaler.fit_transform(x)
x = pd.DataFrame(x_scaled)

X_train,X_test,y_train,y_test=train_test_split(x,y,test_size=0.2)
print(X_train,X_train.shape)

# from sklearn.linear_model import LinearRegression
# from sklearn.metrics import mean_squared_error

# lr = LinearRegression()
# lr.fit(X_train, y_train)

# # Predict the output labels on the testing set
# y_pred = lr.predict(X_test)

# # Evaluate the model using mean squared error
# mse = mean_squared_error(y_test, y_pred)
# print('Mean Squared Error:', mse)

from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
poly = PolynomialFeatures(degree=2)
X_train_poly = poly.fit_transform(X_train)
X_test_poly = poly.transform(X_test)

# Fit the polynomial regression model to the training data
model = LinearRegression()
model.fit(X_train_poly, y_train)

# Predict the output labels on the testing set
y_pred = model.predict(X_test_poly)
for k in y_pred:
  print(k)
# Evaluate the model using mean squared error
mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse)

import tensorflow as tf
from tensorflow import keras
print(X_train.shape[1])
# Define the model architecture
# model = keras.Sequential([
#     keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
#     keras.layers.Dense(128, activation='sigmoid'),
#     keras.layers.Dense(64, activation='softmax'),
#     keras.layers.Dense(64, activation='relu'),
#     keras.layers.Dense(1)
# ])

# # Compile the model with a mean squared error loss function and Adam optimizer
# model.compile(optimizer='RMSprop', loss='mean_squared_error')

# # Train the model on your dataset
# model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))

# # Evaluate the model on the test set
# loss = model.evaluate(X_test, y_test)
# print("Test loss:", loss)

# # Make predictions on new data
# y_pred = model.predict(X_train)
import tensorflow as tf
from tensorflow import keras
from sklearn.preprocessing import StandardScaler

# #Normalize the input data
# scaler = StandardScaler()
# X_train = scaler.fit_transform(X_train)
# X_test = scaler.transform(X_test)

#Define the model architecture
model = keras.Sequential([
    keras.layers.Dense(256, activation='relu', input_shape=(X_train.shape[1],)),
    keras.layers.Dropout(0.2),
    keras.layers.Dense(128, activation='relu'),
    keras.layers.Dense(64, activation='relu'),
    keras.layers.Dropout(0.2),
    keras.layers.Dense(1)
])

# Compile the model with a mean squared error loss function and Adam optimizer
model.compile(optimizer='adam', loss='mean_squared_error')

# Increase the number of epochs and add early stopping
early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)
model.fit(X_train, y_train, epochs=15, batch_size=32, validation_data=(X_test, y_test), callbacks=[early_stop])

# Evaluate the model on the test set
loss = model.evaluate(X_test, y_test)
print("Test loss:", loss)

# Make predictions on new data
y_pred = model.predict(X_train)

# scaler = StandardScaler()
# X_train = scaler.fit_transform(X_train)
# X_test = scaler.transform(X_test)
# !pip install keras-tuner==1.0.2

# import tensorflow as tf
# from tensorflow import keras
# from tensorflow.keras import layers
# from sklearn.preprocessing import StandardScaler
# from kerastuner.tuners import RandomSearch
# from kerastuner.engine.hyperparameters import HyperParameters

# shuffled_indices = np.random.permutation(X_train.shape[0])
# print(len(X_train),len(y_train))
# # X_train = X_train[shuffled_indices]
# # y_train = y_train[shuffled_indices]

# def build_model(hp):
#     model = keras.Sequential()
#     model.add(layers.Conv1D(hp.Int('conv1d_1_filters', min_value=32, max_value=128, step=32),
#                             hp.Int('conv1d_1_kernel_size', min_value=2, max_value=5, step=1),
#                             activation='relu', input_shape=(X_train.shape[1], 1)))
#     model.add(layers.MaxPooling1D(pool_size=2))
#     model.add(layers.Conv1D(hp.Int('conv1d_2_filters', min_value=32, max_value=128, step=32),
#                             hp.Int('conv1d_2_kernel_size', min_value=2, max_value=5, step=1),
#                             activation='relu'))
#     model.add(layers.MaxPooling1D(pool_size=2))
#     model.add(layers.Flatten())
#     model.add(layers.Dense(hp.Int('dense_1_units', min_value=32, max_value=256, step=32),
#                            activation='relu'))
#     model.add(layers.Dense(1))
#     model.compile(optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),
#                   loss='mean_squared_error',
#                   metrics=['mean_squared_error'])
#     return model

# Normalize the input data
# scaler = StandardScaler()
# X_train = scaler.fit_transform(X_train)
# X_test = scaler.transform(X_test)

# # Reshape the input data for CNN
# X_train_cnn = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
# X_test_cnn = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)

# # Define the Keras Tuner
# tuner = RandomSearch(build_model,
#                      objective='val_mean_squared_error',
#                      max_trials=10,
#                      executions_per_trial=2,
#                      directory='my_dir',
#                      project_name='my_project')

# # Search for the best hyperparameters
# tuner.search(X_train_cnn, y_train, epochs=10, validation_data=(X_test_cnn, y_test))

# # Get the best hyperparameters and build the model
# best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]
# model = build_model(best_hp)

# # Train the model on the full dataset with the best hyperparameters
# model.fit(X_train_cnn, y_train, epochs=50, batch_size=32, validation_data=(X_test_cnn, y_test))

# # Evaluate the model on the test set
# loss, mse = model.evaluate(X_test_cnn, y_test)
# print("Test loss:", loss)
# print("Test MSE:", mse)

# # Make predictions on new data
# y_pred = model.predict(X_train_cnn)

print(model.evaluate(X_test, y_test))

# print("Test Loss:", loss)
# print("Test Accuracy:", accuracy)

import os
import sys
import pickle

projectabspathname = os.path.abspath('Untitled7.pickle')
print(projectabspathname)
projectname = 'Untitled7.ipynb'
projectpickle = open(str(projectabspathname),'wb')
pickle.dump(projectname, projectpickle)
projectpickle.close()